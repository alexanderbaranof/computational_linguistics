{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "dvach = open('data/2ch_corpus.txt').read()[:10000]\n",
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "\n",
    "news = open('data/lenta.txt').read()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]\n",
    "sentences_news = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "threegrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    threegrams_dvach.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "threegrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    threegrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = np.zeros((len(bigrams_dvach), \n",
    "                   len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "id2bigrams_dvach = list(bigrams_dvach)\n",
    "bigrams2id_dvach = {bigram:i for i, bigram in enumerate(id2bigrams_dvach)}\n",
    "\n",
    "\n",
    "\n",
    "for ngram in threegrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    \n",
    "    key_bi = word1 + ' ' + word2\n",
    "\n",
    "    \n",
    "    matrix_dvach[bigrams2id_dvach[key_bi]][word2id_dvach[word3]] =  (threegrams_dvach[ngram]/\n",
    "                                                                     bigrams_dvach[key_bi])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим матрицу вероятностей перейти из биграммы в другое слово\n",
    "matrix_news = np.zeros((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigrams_news = list(bigrams_news)\n",
    "bigrams2id_news = {bigram:i for i, bigram in enumerate(id2bigrams_news)}\n",
    "\n",
    "\n",
    "# вероятность расчитываем точно также\n",
    "for ngram in threegrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    \n",
    "    key_bi = word1 + ' ' + word2\n",
    "    \n",
    "    matrix_news[bigrams2id_news[key_bi]][word2id_news[word3]] =  (threegrams_news[ngram]/\n",
    "                                                                     bigrams_news[key_bi])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, id2bigram, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        \n",
    "        current_bigram = id2bigram[current_idx]\n",
    "        current_second_word = current_bigram.split()[1]\n",
    "        \n",
    "        next_bigram = f'{current_second_word} {id2word[chosen]}'\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = bigram2id['<start> <start>']\n",
    "        else:\n",
    "            current_idx = bigram2id[next_bigram]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "много раз пробирался он через линии германских войск неся на себе спрятанными в ошейнике шифрованные депеши \n",
      " с места происшествия запросили 20 машин скорой помощи \n",
      " агентство итар-тасс в сообщении от 21.15 со ссылкой на источники в гувд москвы говорит только о 30 раненых в том числе о двух пострадавших в результате этого взрыва может составить до ста человек \n",
      " среди причин происшедшего называют террористический акт связанный с последними событиями в дагестане однако по сообщению орт не исключается и версия об аварии взорвался игровой автомат \n",
      " когда показывался аэроплан-птица красиво и вольно паривший в воздухе австрийцы указывали — das ist\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news, id2bigrams_news, bigrams2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "второй опенинг параша но дискасс \n",
      " так и должно быть то ближе к середине я не понимал почему он не становится брутальным дротикометателем \n",
      " и наверное пока единственный \n",
      " ну к сайто у меня так ебать кожа на ебле шелушится шо делать где можно скачать сей тайтл \n",
      " цундере не нужны \n",
      " тоха cinderella cage принцесса лучше всех \n",
      " и мне и гатари нравятся как и тамошняя манера подачи просто я считаю что автор мысль доносил на примере слабоватых примеров вот и все \n",
      " даешь хенриетту с сайто на свадьбу и хэппи энд смотрел \n",
      " освятил шедевром японской анимации эта\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, word2id_dvach, id2bigrams_dvach, bigrams2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian') + [\"это\", \"весь\"])\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def normalize(text):\n",
    "    tokens = re.findall('[а-яёa-z0-9]+', text.lower())\n",
    "    normalized_text = [morph.parse(word)[0].normal_form for word \\\n",
    "                                                            in tokens]\n",
    "    normalized_text = [word for word in normalized_text if len(word) > 2 and word not in stops]\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text)\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "\n",
    "def ngrammer(tokens, stops, n=2):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data/lenta.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count):\n",
    "    if bigram_count >= min_count:\n",
    "        corpus_word_count = float(corpus_word_count)\n",
    "        pa = worda_count / corpus_word_count\n",
    "        pb = wordb_count / corpus_word_count\n",
    "        pab = bigram_count / corpus_word_count\n",
    "        try:\n",
    "            return np.log(pab / (pa * pb))\n",
    "        except ValueError:\n",
    "            return -100000\n",
    "    else:\n",
    "        return -100000\n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['бой', 'сопоцкина', 'друскеник', 'закончиться', 'отступление', 'германец'],\n",
       " ['неприятель',\n",
       "  'приблизиться',\n",
       "  'север',\n",
       "  'осовца',\n",
       "  'начать',\n",
       "  'артиллерийский',\n",
       "  'борьба',\n",
       "  'крепость'],\n",
       " ['артиллерийский', 'бой', 'принимать', 'участие', 'тяжёлый', 'калибр']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=0)\n",
    "p = gensim.models.phrases.Phraser(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=0)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить',\n",
       " 'взрыв_произойти',\n",
       " 'третий',\n",
       " 'уровнечетвертый',\n",
       " 'ярус',\n",
       " 'комплекс',\n",
       " 'зал',\n",
       " 'игровой_автомат']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=5)\n",
    "p = gensim.models.phrases.Phraser(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=5)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить',\n",
       " 'взрыв',\n",
       " 'произойти',\n",
       " 'третий',\n",
       " 'уровнечетвертый',\n",
       " 'ярус',\n",
       " 'комплекс',\n",
       " 'зал',\n",
       " 'игровой_автомат']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=10)\n",
    "p = gensim.models.phrases.Phraser(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=10)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['установить',\n",
       " 'взрыв',\n",
       " 'произойти',\n",
       " 'третий',\n",
       " 'уровнечетвертый',\n",
       " 'ярус',\n",
       " 'комплекс',\n",
       " 'зал',\n",
       " 'игровой',\n",
       " 'автомат']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2[p[corpus[333]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из примера видно, что при пороговом значении 0 были обе биграммы, при значении 5 осталась одна, а вторая не прошла, а при значении 10 не прошли обе биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
